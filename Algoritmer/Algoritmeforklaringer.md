# Algoritmeoppgaver som kan komme på eksamen

## Pattern matching
### Brute Force
### Boyer-Moore
### Knuth-Morris-Pratt
## Tries
## Huffman coding
## Traveling salesperson
### Double Tree Algorithm
### Christofides' Algorithm
## Minimizing the Sum of Completion Times on a single machine (Chapter 4.1)
## MAX SAT
### Randomized Algorithm
### Derandomization
## Formulate ILP, LP and Dual given an instance of
### Set cover
#### Linear Programming
* Plays a central role in design and analysis of approximation algorithms
* Important Terms
    * **Decision variables**: represent some sort of decisions that need to be made
    * **Constraints**: Variables are constrained by a number of linear inequalities  
    * **Feasible solutions**: Any assignment of real numbers to the variables such that the constraints are satisfied
* Exists efficient algorithms to solve LP 
* Example
![Linear programming](res/LPex.png)
#### Integer Programming

* Integer linear programming
    * Exclude fractional solutions
* Zero-One linear programming
    * Allow only 1 and 0 as values for decision variables
* Integer Programming as NP-Hard 
### Vertex cover

## Coloring Graphs(vertices)

### 2 coloring algorithm

### (delta+1) coloring algorithm

### the O(sqrt(n)) given Chapter 6.5 (and also in Compulsory 2)

# Eksempelspørsmål ML From slide

## What is machine learning
Algorithms whose performance improve as they are exposed to more data over time. Performs task without us specifically 
programming explicit solutions. Machine learning exceeds in difficult to program problems. Where we would struggle to 
make a well-defined solution. The data might be to complicated or there is simply to much of it. Machine Learning provides
a flexible solution. Programs that adapt to their inputs instead of rigid pre-programmed solutions.

## Why do a train-val-test split
In  ML we're building a model to predict something for new measurements. To simulate having new measurement we need to 
leave out  a set of data when constructing the model. We use this test to test the model and is therefor called a test set.

The rest of the set is train set. This is what we train the model on. The bigger the better for training our model. 
If it's bigger we get bigger deviation and the model can therefor generalize a bit more. If we use the entire data set 
the model can typically "remember" the data and cannot predict new data as successfully.

In machine learning we provide Data and answer to the data and it outputs rules for new data in the same category.
Det er meir representasjon av features enn data direkte i konvensjonell maskinlæring. I deep learning derimot blir direkte
data gitt som input.
 
 Klassisk maskinlæring blir brukt til å finne ting som kreft og lignende.
 
 Three main types of machine learning
 1. Supervised learning
    * Learn a model from labeled training data. Used to make predictions on unseen future data.
    * Goal: Learn connections between input and desired output. Function approximation.
    * Example application: will the customer click my ad?
    * 99% of the value generated by ML is based on supervised learning.
 2. Unsupervised learning
    * Find meaningful patterns in data, without feedback
    * Clustering, anomaly detection.
    * Example application: Grouping customers based on their purchasing history.
 3. Reinforcement learning
    * An agent gathers information from its environment and learns to select actions that maximize a reward.
    * Think: mouse in a maze trying to get the cheese.
    * Example application: AlphaGo
    
Machine learning slightly with development of important statistical techniques and optimization tools in 1650s. And 
continued in the 1940s with Cybernetics. Theories of biological learning.

 
    
### Supervised ML

Some ingredients:
* Data
    * Feature vectors
    * Images
    * Speech
* Labels/annotations
    * {cat, dog, horse}
    * {cancer, !cancer}
    * {coalfish, pollock}
    * {male, female}
* Training data
    * Pairs {(data, label)}
    * Input to ML model
* Trained model
    * A function
        * Sample -> label
* Measure of success
    * Is it doing a good job?
    * Accuracy, loss, ...
    * Used as a feedback signal
* Purpose...
    * What's the model for?
    * Broader context
    * Business impact
    
## Classification
* Cross-Validation
* Confusion matrix
* Precision and recall
* ROC curves
* more stuff

In Classification we want to decide which of the N classes an input belongs to (think of the fish from the slides in the
first lecture(racist elmo??????)). 

Regression is about finding a function that maps from the inputs to a continous set of numbers:

f: X -> R

Think of the housing prices predictoin from assignment 1, where you're constructing a function mapping from a set of 
features of a housing district to the price of a house in that district. Or more generally, say you have set of x-values
and corresponding outputs as real numbers. when given a new, previously unsees x-value, what y-value should you predict?
What is the best interpolating function
 
### Classification vs Regression
Is there continuity on the possible predictions? Predicting a price of 1 500 000 vs 1 500 001 makes a very small difference  

### Confusion matrix
Confusion matrix is a table that summarizes the results from a classification model's predictions. The idea is to count
the number of times instances of a certain class is classified as the various classes in your problem. So in MNIST case:
how often the 4s and non-5s are correctly classified, how often a 5 is misclassified as a non-5 and how often a non-5 as
a 5. We need to produce some predictions, this can be done with ``cross_val_predict`` which returns predictions based on
all the K folds.

![Confusion matrix](https://www.researchgate.net/profile/Alejandro_Baldominos/publication/328816477/figure/fig3/AS:703352962809858@1544703822325/Confusion-matrix-for-the-MNIST-dataset-with-the-best-ensemble.png)

### ROC curves

ROC curve(receiver operating characteristic curve) is a graph that's often used to evaluate binary classifiers. It's a 
plot of the true positive rate vs the false positive rate. TPR = TP/(TP+FN), FNR = FP/(FP+TN). Sklearn has a function
for this calles ```roc_curve```. A really good classifier will have a ROC curve that's very close to the top left corner.
This means it has high TPR and low FPR for any thresholds. We can use ROC curves to compute how good a classifier is.
If it's good there's a high area under the curve. If it's bad the area will be close to 0.5. This is called ROCAUC, ROC
area under the curve.

## What can go wrong if you tune hyperparameters based on test-set performance
The test set is usually a small set and this would cause a lack in generalization. The model would then probably miss a 
lot on new data.

## What is "generalization" in machine learning
Generalization is the ability to predict new data coming in. Data has some general deviation and an overfitted or 
overtrained model. Or a model trained on a to small set will fail at generalizing new data coming in and miss on it.
 
## What is cross-validation
This is achieved by splitting the training set into several parts called folds. Say K folds. Then we will train the 
model 3 times. Each time we use a different fold for evaluation and training on the remaining K-1 folds. The average 
score for the K runs is used to estimate the model's performance. This means that each set is part of the training set 
K-1 and part of the evaluation set once. An important advantage of this approach over the one above(splitting into two 
separate sets) is that it doesn't waste as much training data. Unless you have plenty of data, cross validation is the 
preferred method for estimating model performance.

Cross validation also provides a more thorough test than splitting the data into a training set and a test set. The 
```train_test_split``` procedure sets aside a fixed random subset of the data as a test set. If we're unlucky, all the 
difficult examples end up in the training set, while the test set contains only easy ones.

NOTE: the special case of cross-validation where K is set to the number of data points in the training set is called 
leave-one-out. Each fold is then a single sample.

The result of this K-fold cross validation procedure is an array of K evaluation scores.    

## Validation set and model selection
It's very important to not base decisions on the parameters in model selection based of test set performance. That is 
because the model will be biased based on the test set. So to get get an unbiased estimate we split the training set into
two sets, a data set used for training and another set for evualuating performance while trying out various possible 
models and settings, called a validation set. 

## Explain concept of "underfitting", "Overfitting" and "generalization" in machine learning
underfitting is when the model is not trained enough and will not hit accurately enough because of purely missing or 
over generalizing. Overfitting is when the model is trained to much on the selected model and will therefor hit the 
dataset it's trained on more or less perfectly but will miss on new data. Generalization is mostly what we're after.
Generalization in machine learning is essentially how well the model responds to new data coming in. We want a model that
generalizes enough to hit new data and not so much that it completely misses.

## What is precision? What is recall?
The **recall** of a binary classifier is the proportion of actual positives that were correctly identified. In other words, 
recall = true positive/all actual positives = TP/P = TP/(TP+FN).

The **precision** of a binary classifier is the proportion of the positive predictions that were actually correct. In 
other words, precision = true positives/positive prediction = TP/(TP+FP). Sometimes called true positive rate

### More precision and recall

**Specificity** of a binary classifier is the proportion of actual negatives that were correctly identified. Sometimes 
called true negative rate


### Binary classifier
A binary classifier is essentially a system that distinguish one thing from another. Like is this number a five or not a
five. 

### Accuracy and different sorts of errors
If we classify something as belonging to the positive class we can either be correct(**true positive**) or incorrect
(**False positive**). If we classify something to be negative we can either be correct (**True negative**) or incorrect 
(**False negative**). What errors we care the most about depends on the task: if we're for example diagnosing a treatable
condition in patients we should do everything we can to reduce the rate of false negatives. While perhaps still keeping 
an eye on the false positive rate because a positive diagnosis could lead to invasive and extensive further testing of 
the patients.

In spam filtering we care most about not marking important emails as spam, i.e. we want a low false positive rate(non-spam
marked as spam) even if it means a higher false negative rate(some spam emails ending up in our inbox).


## What is the so-called precision-recall tradeoff?
It's typically impossible to achieve both high precision and high recall simultaneously. They are typically competing, 
when one is high the other is low and vice versa.

Are false negatives very bad(like in medical diagnosis)? select a low threshold to get a high recall and OK precision. 
Are false positives especially costly(like in spam detection)? Go for a threshold that gives you high precision and OK recall. 

"If someone says "Let's reach 90% precision", you should ask, "at what recall?""

## Training models
### Linear regression
In regression we seek to identify a continuous variable y associated with a given input vector x.

In dumb dumb: We try to find the best straight line through that goes through the points.

y = b + w * x

* y is the dependent variable
* x is the independent variable
* b and w are the constant variables
    * b is called intercept
    * w is called coefficient or slope
    
![Linear regression](https://media.geeksforgeeks.org/wp-content/uploads/linear_regression_result-1.png).

**Application of linear regression**

* Stock market forecast
    * f(past price, size of market, ...etc)= Dow Jones stocks for tomorrow(increase or decrease)
* Self-driving cars
    * f(sensors) = angle of the steering wheel
* Recommendation
    * f(user A, product B) = probability of user A buying product B

Loss-function is sum of square error. We do this to minmize the error to best fit the data points.
    * How differences between the predicted value and the actual value.
    * Example: quadratic loss function, absolute loss function, logarithmic loss function, ...etc
    
**Mean square error**(MSE): ``from sklearn.metrics import mean_squared_error``

**Least square method**(LSM): ```from sklearn.linear_model import LinearRegression```
 
**Root mean square error**(RMSE): ``import math``

**Mean absolute error**(MAE): ``from sklearn.metrics import mean_absolute_error``

**R-squared**: ``from sklearn.metrics import r2_score``

If we see that the points make a slope or the form of the points is bending a lot we can use polynomial regression.

There's also something called multiple linear regression for when we are comparing more than two variables.


### Gradient descent
* An iterative way to find the minimum of the function
* Optimization of function (hyperparameters)
* Optimized the parameters using the gradient descent in the deep learning neural networks
    * Minimum of the function
* Two steps
    * Compute the slope that is the derivative of the cost function at the current point
    * Move in the direction of the slope increase(or decrease) from the current point by the computed amount.
   
Gradient descent considers a loss function.
* We start by picking an initial random point.
* Compute the differential(slope) of the point to the loss function. 
    * If the answer is negative we increase the point
    * If the answer is positive we decrease the point
* We have to consider the learning weight.
    * A too large and constant learning rate will overshoot
    * A small and constant learning rate will take it's goddamn time to reach the minima.
    * A gradient learning rate will hopefully take big steps in the start to get as close as possible and make smaller 
    steps to get as close as possible to the minima without overshooting or undershooting
    
![Visualization of local/global minima](https://www.i2tutorials.com/wp-content/uploads/2019/09/Neural-network-32-i2tutorials.png)

There can be both local and global minimas.

We terminate the gradient descent either by:
1. Convergence
2. Number of iterations.


**Batch gradient descent**:
* The parameters are updated once after all the training examples have been passed through the network.
    * 100 training examples then the parameters of the neural network are updated once after all examples are determined.
* **Good at**:
    * It produces a more stable gradient descent convergence and stable error gradient than stochastic gradient descent.
* **Not good at**:
    * Somethimes a stable error gradient can lead to a local minima
    * The entire training set can be too large to process in the memory
    * It takes too long for processing all the training samples as a batch
    
**Stochastic gradient descent**:
* One training sample (example) is passed through the neural network at a time
* Parameters (weights) of each layer are updated with the computed gradient

* We change one-by-one each time

sklearn has an built in function for this: ``from sklearn import linear_model``

``clf = linear_model.SGDRegressor(max_iter==10000, eta0=0.0001)``

* **Good at**:
    * It is easier to fit into memory due to a single training sample being processed by the network
        * One sample is processed at each time
    * It can converge faster
        * Updates to the parameters more frequently
    * Help getting out of local minimums of the loss function
* **No good at**:
    * Due to frequent updates the steps taken towards the minima are very noisy
        * This can often lead the gradient descent into other directions
        * Longer achievement

**Mini-batch gradient descent**: 
* This is a mixture of both stochastic and batch gradient descent
    * 100 samples, examines 10, 20, 30..., etc., at each time
* Find the balance between BGD and SGD

A bunch of samples are updated at each iteration

* **Good at**:
    * Easily fits in the memory
    * It is computationally efficient
    * If stuck in local minimums, some noisy steps can lead the way out of them.

#### Setting learning rate
Learning rate is very important and it is hard to set a good rate. An adaptive rate is good for this. At the beginning,
learning rate should be set as a larger rate since we would like to achieve the optimized solution ASAP. After several
iterations, we are approaching to the destination, the learning rate should be reduced. Different parameters should have 
different learning rates.

**Adagrad** is an adaptive gradient algorithm. Consider to assign the learning rate to each parameter, individually. 
Learning rate is decreased as the time goes by. As the increase of the steps, we hope learning rate can be converged

Tensorflow is an library full of optimization methods.

Feature scaling is also a thing. Seems to scale down.

#### Evolution computation computation
* Holland's original GA is now known as the simple genetic algorithm(SGA).
    * Natural inspiration
    * Stochastic search
* Genetic algorithm is used to find the optimized solutions
* Three operations in  GA
    * Mutation
    * Crossover
    * Selection

Crossover(same feature with both is the same) rate is much higher than mutation rate(something random)

* Selection
    * Select the best and suitable mate for yourself
        * Fitness function
        * is different in varied domains and applications
* Crossover
    * Marriage for next generation
    * Inherit partial features from parent
        * 30% from dad, 70% from mom
* Mutation
    * Get better solution
        * Become stronger to adapt to the environment


### Learning curve
**So far:**

* Training a model based on the training data
    * Get the parameters
        * LSM
        * Gradient descent
        * ...
    * What is the main purpose
        * Knowing the predicted results of the testing data to evaluate whether this model is good

We need to know the error of the new testing data and find the best function from the function set.


#### Bias and variance

* **Bias**
    * High bias
        * Pay very little attention to the training data and oversimplifies the model
        * It always leads to high error on training and test set
* **Variance**
    * High variance
        * Pay a lot of attention to the training data and does not generalize on the data which it hasn't seen before
        * Model performs very well on training data but has high error rates on test data

If the model is simple the variance is low, if the model is complex the variance is high. If the model is simple, the 
result has less influence by the sample data.

* Large bias, underfitting
    * The designed model cannot even fit a training data
* Large variance, overfitting
    * You fit the training data, but has the large error on the testing data

* If the model is underfitting, the model is too simple, the bias is high
    * Re-design the model
        * Add more features into input
            * Weight, height so on
        * A more complex model
            * x squared, x cubed etc.
* If the model is overfitting, the model is already complicated, high variance
    * Add more training data
        * It cannot always be done
    * or regularization

#### Model selection
There's a trade-off between bias and variance. Select a model to minimize the error in the testing data. How do we select
a good model?

Cross validation by splitting into training, validation and testing by using sklearns ``train_test_split``. This way we 
get 60% training, 20% validation and 20% testing.

can also use sklearn ``cross_val_score``.

#### Learning curve
* Learning curves are deemed effective tools for new monitoring the performance of workers exposed to a new task
* Train learning curve
    * Learning curve calculated from the training dataset that gives an idea of how well the model is learned.
* Validation Learning curve
    * Learning curve calculated from a validation dataset that gives an idea of how well the model is generalized.
    
#### Problems in machine learning 
* Optimization
    * Find the best function to obtain the optimized parameters for the training data.
    * The predicted model cannot be used for all cases
    * Overfitting problem
* Generalization
    * Find the best function to fit all the testing data
    * The parameters cannot be optimized
    * Underfitting problem
* Solution
    1. Start training
    2. High training error?
        * Yes, underfitting high bias
        1. Training longer
        2. Training under more complex model
        3. Obtain more features
        4. decrease regularization
        5. New model
    * High validation error?
        * Yes, overfitting high variance
        1. Obtain more data
        2. Decrease number of features
        3. Increase regularization
        4. New model
    * End 

    
### Regularization
* Instead of re-design the model 
    * we can do regularization. 
* Keep the current features, but reduce the influence of the un-important features
    * Generalization for the coming new data
        * Shrink the importantness of coefficients
    * Good for a model with many features

As you know, restricting models can be used to reduce their variance and therefore their tendency to overfit.

Following parameters are used for regularization
* ``Max_depth``: As we saw above, reducing the maximum depth a decision tree is allowed to have, one can simplify the 
decision boundary and therefore prevent overfitting. Decrease to regularize.
* ``min_samples_split``: This parameter controls the minimum number of samples a node can have to be allowed to split.
Increase to regularize. 
* ``min_samples_leaf``: The minimum number of samples a leaf must contain. Increase to regularize.
* ``max_features``: The maximum number of features that are evaluated when deciding whether to split each node. Decrease
to regularize.
* ``max_leaf_nodes``: The maximum number of leaf nodes. Decrease to regularize.
* ``min_impurity_deacrease``: Split nodes if the split results in a decrease of the impurity greater than or equal to 
this value. Increase to regularize.
* ``min_impurity_split``: Split a node if its impurity is above this threshold. Otherwise it's a leaf. Decrease to 
regularize.

#### Ridge regression
* Advantage
    * To solve the problem of overfitting
    * What is overfitting problem?
        * Fit the training data really well but worse in testing
    * Purpose
        * Generalize the model to have better performance on both training and testing
        * Add the penalized term to overcome the overfitting
        * Make the model more generalized, flat, and horizontal
* Loss function
    * LSM (Sum of squared residuals)

Ridge regression line has less slope than the least square line, Less sensitive on X

* Shrinking the coefficients leads to a lower variance and in turn a lower error value
* Decrease the complexity of a model but does not reduce the number of variables, it rather just shrinks their effect
    * It is hard to be 0 for the coefficients, but tends to be a very small number
    
get a good lambda with trial and error. Try several values for it, use 5-fold or 10-fold cross validation to see which
value can make a good results in lower variances

The best situation to use the ridge regression is when the sample data is small, it can make a better prediction to 
generalize the model for the testing data or prediction is less sensitive to the training data. Ridge regression starts 
from a small and worse fit to the training data, but has better predictions for the long term

#### Lasso regression
Absolute values of the slope. less variance than the least square line. less sensitive of X to Y for a small training 
data. High lambda makes a lower slope.

* **Ridge regression**
    * Shrink the slopes (w_3 and w_4) close to 0
    * Shrink a lot but it is hard to be 0
* **Lasso regression**
    * Shrink the slopes (w_3 and w_4) all the way to 0 if it is not relevant to final results.
    * High possibility shrink to 0

Lasso regression is similar to ridge regression, however given a suitable lambda value lasso regression can drive some
coefficients to zero

* Eliminate some features entirely
    * Feature selection

#### Elastic regression
**Lasso** can exclude the useless variables, thus reducing the variance in the models if it contains many useless 
variables.

**Ridge** is doing better when most variables are useful.

Elastic net is kind of a mix between Lasso and Ridge


#### Logistical regression
* Linear regression
    * Predict a numerical value for the coming event
        * Stock price will be increased/decreased in the coming two weeks
* Logistic regression
    * Predict the class for the coming event with the probability
        * SPAM email or not
    * It is not relevant to regression model but more like classification
        * Binary results, 0/1 (False/True)
    * Supervised learning
    * Trying to find the S shape
        * The curve goes to 0 to 1
    * If the weight of a person is very high
        * He/She has a high probability to get diabetes
    * If the weight is medium
        * 50% chance to get the diabetes
    * If the weight is low
        * Lower change to get the diabetes
    * This is a classification problem, thus
        * If a change is larger than 50%, we will say he/she has the diabetes
        * Otherwise, no
    * Same as the regression model, the diabetes can be predicted by the weight
    * Different to the linear regression
        * How does line fit to the data
    * Linear regression
        * Find the minimum the sum of the squares of the residual errors
            * MSE, RMSE, R-squared
    * There is no residual for the logistic regression
        * Cannot calculate MSE, RMSE, R-squared
    * Use maximum likelihood
        * Calculate the likelihood of each point
        * Multiple all the likelihood together
            * The likelihood of the data given to this line
            
The curve with the maximum likelihood is selected

Built in functions
* ``from sklearn import linear_model``
* ``linear_model.LogisticRegression()``

**Decision boundary**

Decision boundary helps to differentiate probabilities into positive class and negative class

##### Linear regression problem
* Linear regression can be used for the prediction
    * y = b + wx
        * Given a weight, predict the diabetes

#### Softmax regression
* Softmax regression allows for the classification of an input into discrete classes
    * Logistic regression provides only binary classification
    * Multiple classification
* Softmax allows for classification into any number of possible classes
    * Probability
        * Must equal to 1
* Two steps:
    * Sum up all the possible features of each class
    * For all class, find the normalized probability of each class
* Built-in function
    * ``from mlxtend.classifier import SoftmaxRegression``
    
## Decision trees, random forests and gradient boosted trees
What are the best machine learning models in existence?

* Gradient boosted trees (XGBoost)
* Deep neural networks (Deep learning)
    * Recommendation systems
    * OpenAI
    * Picture recognition
    * Speech recognition

### Decision trees
Decision trees are full of nodes asking simple questions like is x > 5 and so on. In a plotted system each node splits 
graph. In otherwords decision trees are a hierarchy of if/else questions. 

* How to grow a tree
    * Search over all possible if/else tests for all possible features and split using what's most informative
    * Continue recursively
    * Until each leaf of the tree contains only one class (or until a stopping criterion is met)
* How to make predictions
    * Let the new data point traverse through all the questions until it ends in a leaf
    * Predict that it belongs to the majority class in the leaf
    * For regression: predict the mean value of the training points in that leaf

We can let the models vote on a prediction
    * Logistic regression
    * SVM Classifier
    * Random Forest Classifier
    * Others...

Split training set into different smaller sets and train on those. Out of this you get predictors (e.g classifiers).

* Random sampling with replacement = bagging
* Random sampling without replacement = pasting

Decision trees with bagging makes more sense and generalizes a bit more.

Boosting note: Boosting increases the weight of some points in the graph.

We can also use different predictions and blend them to one.

* **Root node**: The top node. No parent node. Where you start. Poses the first if-else question.
* **Internal node**: Has one parent node and two children. These nodes pose if-else questions.
* **Leaf node**: A node without children. Not possible to go any further. A bottom node. These nodes provide predictions.
* ``samples``: The number of training samples that the node applies to. The number of training instances that end up 
passing through the node. for example, there are 179 samples for which the glucose is less than 154.5
* ``value``: A list of the number of samples of each class that the node applies to.
* ``gini``: A measure of "impurity". The gini is 0 if all the training samples in that node belongs to a single class. 
It grows towards 1 as the class diversity at the node increases

We'll have more to say about the Gini impurity shortly, as it's what's used to make decision trees.

But first, it's instructive to visualize the decision boundary produced by our decision tree, as we did back in Part 3.

Decision boundaries can be shown in a graph as a line that splits the graph. The boundary is not necessarily linear and 
not smooth as for polynomial regression.

NOTE: The decision boundaries of decision trees are always parallell to an axis. Why is that? It's because each node in 
a decision tree consists of a test using only one feature at a time.

#### Gini and information gain

Decision trees learn the relationship between the training data and the corresponding labels by organizing the training 
data in a binary tree. Each leaf in the tree makes a specific prediction. Each internal node compares a single feature 
value against a single threshold, and places instances in their corresponding child nodes.

Training a decision tree is done recursively, roughly as follows (we'll study training more carefully below):
* The goal is to obtain leaves that are as pure as possible, with the maximum amount of samples in the leaves belonging 
to the same class.
* Starting at the root node, the data is split into two parts based on the feature that has the larges information gain.
* This is repeated for each child of the node, and so on in an iterative way.

To achieve this we need a measure of information gain, and we also need it to tell us hen a node is pure.

**Gini impurity** is one such measure. Gi = 1 - sum of Pik squared.

P is the proportion of the samples that belongs to class K for the node i

We note that:
    * If the node is pure, that is all the samples belong to a single, class then Gi = 0
    * If a node has many samples belonging to each of many classes, then Gi gets closer to 1. Very impure.
    * The information gain of a node can be computed as the Gini impurity at the node minus the sum of the Gini impurity
     of its children weighted by their sizes. 

Decision trees have the ability to output predictions and estimated probabilities. So decision trees can say "The model
predicts that you have diabetes with 74% probability"

**Cart algorithm** searches greedily for a feature and a threshold that gives the highest information gain when used to 
split the data. For example, glucose and glucose less than 132.5.

Once it has split the training data in two at the root, it continues in the same way on each of the two subsets.

**Cost function** that CART tries to minimize is the node impurity. Calculated by number of samples in the left samples 
divided by total number of samples in the node + the same for the right node. The algorithm stops once it reaches the 
``max_dept``, or if it's not possible to find more splits that reduce the impurity. Or when another stopping criteriu, 
for minimum number of samples in a node, is reached. 

#### **Instability**

As you've realized, decision trees are great machine learning models in many ways. They are

* easy to user (work for any combination of categorical and continuous features, without need for scaling or other 
pre-processing).
* easily interpretable (explaining why a decision tree made a decision is simply a matter of retracing the steps taken 
by the data through the tree, The possibility of computing feature importances is also super handy for interpretability)
* can easily fit the training data.

There are some important downsides. We've seen them tend to overfit regularly if not regularized. They are also extremely
sensetive to small variations in the training datat

To combat these limitations while retaining as many of the nice aspects of decision trees as possible, one usually 
ensembles multiple trees into a single model called random forests. 

### Random Sampling

### Random forests

The negative side about binary trees is that it heavily overfits, if not regularly. They are also extremely sensitive 
to small variations in the data. This is where random trees comes in. We ensemble multiple trees into one model, where 
one of these is called random forests. Random forest also performs much better. Random forest is related to the concept
of "wisdom of the crowd" where a crowd of non-expert that combine their predictions outperform individual experts. 

So to summarize, an ensemble of binary threes which outperforms, is less sensitive and generalizes more than binary 
threes. 

Let's say we want to make 500 trees based on a training set containing ``n_samples`` point. Each tree is built in the
following way:

1. Each tree trained on different data sets: Choose one data point at random for the training data, with replacement. Do
this ``n_samples`` times, that is until you have ``n_samples`` points. This is called bootstrap sampling.
2. Each tree trained on different features: Train a decision tree on this data set, but at each node of the tree select
a random set of features to consider when splitting, up to ``max_features`` of them.

this is a random forest 

Predictions are done differently for regression and classification

* **Regression**: Let each tree make a prediction, then average them.
* **Classification**: Let each tree make a prediction, and use a soft voting strategy to combine them. As we saw in the 
decision trees notebook, each tree provides a probability for its prediction. Average these probabilities and predict 
the class that has the highest average probability

![Random forest classification](http://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1526467744/voting_dnjweq.jpg)

**How to improve models?**

1. Collect better data. Perhaps the problem was framed wrong during data collection? is the data collected in a prudent 
way?
2. Get more data. Real data, synthesized data, or similar data.
3. Create new, better features. Also consider removing uninformative features(these can reduce performance by increasing
 data size requirements)
4. Preprocess the data better. Scaling, transformations, etc
5. Use a better adapted model. Most ML models can be tuned by selecting better hyperparameters. They can be made more or
less complex, which is useful when trying to approximate the true function of a given complexity. "Everything should be 
made as simple as possible, but not simpler"
6. Use a different class of model. Perhaps you've used a model that cannot become sufficiently complex (e.g. logistical 
regression on a complicated data set)? Or perhaps your model is too complex?
7. Use multiple models together in an ensemble. For example using hard or soft voting. Or let each model be based on 
slightly different features than the others, making the models more independent of each other. You can also use a model 
to learn how the ensemble should best be combined, or sequentially train models trying to predict the errors of the 
previous models and then combine them to produce predictions.



**Note: read about the example for random forest for regression and classification**
#### Feature engineering
Some features are more important than others. Some describe the patients from an example more expedient way than others.
That is, they are of greater value for our predictive model.

One of the most important parts of ML is feature engineering

Machine learning is, basically, about function approximation. You want to find a function that is as close as possible 
to the true function mapping from the set of features to correct label or value.

By describing the raw data with new, more suitable features it is possible to reduce the complexity of the true function.

The best way to do feature engineering is to influence how the data is collected. Then it's possible to add new features
that one believes could be useful for modelling. 

#### Ensembling
Ensembling is a general technique for combining machine learning models to make even more powerful models.

There are several variants of this. **Voting, Bagging, boosting, stacking** are some.

Some of the widely used models made using ensembling are random forests, based on bagging, and gradient boosted decision
trees, based on boosting.

## Boosting

boosted trees is one of the most powerful classes of machine learning. Boosting takes another approach to ensembling.
New members are added to the ensemble sequentially_, each new model is trained on the _erros of the ensemble constructed
so far, iteratively learning from its mistakes. We'll focus on boosting based on decision trees, as these are the most 
common base-models

One of the main boosting techniques is **AdaBoost**. In AdaBoost each additional tree focuses on the examples(i.e. 
_instances_) that were misclassified by the previous trees.

In gradient boosting, each tree added to the ensemble tries to predict residual error from the previously added tree. 
So it's trained to predict the difference between the correct value and the value predicted from the ensemble so far.

Each tree is a weak learner, not able to fit data very well. By combing them by having each tree try to predict the 
residual of the previous trees predictions, the result is a very strong learner. Some of the strongest learners we know
are made this way.

#### AdaBoost: adaptive boosting

Introduced in 1995 and the first boosting algorithm with practical use cases. Authors won a prize for the algorithms 
"_elegance, simplicity of implementation, its wide applicability, and its striking success in reducing errors in 
benchmark applications even while in theoretical assumptions are not known to hold_". 

**Basic idea:**
* Give each instance in the training data an _instance weight_, initially set to 1/m where m is number of instances
* Extract a sample of the training data and train a _weak learner_ on it. The weak learners in AdaBoost are typically
decision trees of depth 1(decision stumps), but other models can be used.
* Calculate weight for the learner, based on its error rate. Higher accuracy means higher weight.
* Then increase the weight of the misclassified instances according to the learners weight. An instance misclassified 
by a high-weight learner gets higher weight than one misclassified by a low weight learner.
Extract another sample from the training data, sampled according to the instance weights.
* Train the next weak learner on the sampled data, compute the weight and update the weights of the training instances
accordingly.
* Continue until perfect fit or ``n_estimators`` have been trained.

Each new learner added to the ensemble corrects the shortcomings of previous models as it focuses on high weight instances.
In other words, the weighing of the training data makes the new learner focus more on difficult cases.


### Gradient Boosting
Gradient boosting has the same objective as AdaBoost by correcting its predecessors, but rather than tweaking the instance
weights, each new learner attempts to predict the residual error of the ensemble.

**_Residual errors_**: The basic idea behind gradient boosting

**Boosting:** a way to turn a set of weak learners into a strong learner through ensembling

Random forests is an example of boosting used on decision trees. Adaboost is another approach we'll study, where learners 
added to the ensemble are created to correct the errors of the ensemble so far.

**Residual learning:** Another way to add learners to the ensemble that makes them correct the errors of the ensemble

**Gradient boosting:** Through various clever tricks, one can add trees by fitting them to the gradients of loss functions
computed on the earlier tree in the ensemble. Then produce the next tree in the ensemble by applying gradient descent.

## Name some features. What are the classes? binary trees
Features in binary trees are usually the difference "categories" one example is level of glucose, BMI and age. In the 
example from last lecture some features are worst radius, worst concave points, texture error and smoothness error.  

The class is somewhat of a result. There are classes in each node, but the class is first assigned in a leaf node. 
## Say you have an instance with "worst radius" 17, "texture error" 0.5 and "worst concavity" 0.2. What will be the predicted class? What probability will the model assign the instance
False, False, False. This means the predicted class is cancer, but the gini of the class is just 0.48 which is very high.

The probability for not cancer is 2/5 and cancer is 3/5 

## How are predictions from a random forest produced?
This depends on whether it's regression or classification,

* **Regression**: Let each tree make a predicition and then average them.

* **Classification**: Let each tree make a prediction, and use a soft voting strategy to combine them. As we saw in the 
decision trees notebook, each tree provides a probability for its prediction. Average these probabilities and predict 
the class that has the highest average probability

## What is the idea behind gradient descent
First off gradient descent is a very generic optimization algorithm capable of finding optimal solutions to a wide range
of problems. The general idea of Gradient Descent is to tweak parameters iteratively in order to minimize a cost 
function.

## Can you name some variants of gradient descent? What are their respective strengths and weaknesses?
* **Batch Gradient Descent**:
The parameters are update once after all the training examples have been passed through the network. example: 100 Training 
examples then the parameters of the neural network are updated once after all examples are determined. **Pros**: Batch 
is good at producing a more stable gradient descent convergence and stable error gradient than stochastic gradient 
descent. **Cons**: Sometimes a stable error gradient can lead to a local minima. The entire training set can be too 
large to process in the memory. It takes too long for processing all the training samples as a batch.
 
* **Stochastic Gradient Descent**: One training sample is passed through te neural network at a time. parameters(weights)
of each layer are updated with the computed gradient. **Pros**: It is easier to fit into memory due to a single training 
sample being processed by the network. It can converge faster because it updates to the parameters more frequently. Help 
getting out of local minimums of the loss function. **Cons**: Due to frequent updates the steps taken towards the minima 
are very noisy. This can lead the gradient descent into other directions. Longer achievement.


* **Mini-Batch Gradient Descent**: This is a mixture of both stochastic and batch GD. 100 samples, examines 10, 20, 30,...
at a time. Find the balance between BDG and SDG. A batch of samples are updated each iteration. **Pros**: Easily fits in 
the memory. It is computationally efficient. If stuck in local minimums, some noisy steps can lead the way out of them.


## What are neural networks? Name some ingredients
Neural networks are set of algorithms that are modeled loosely after the human brain. The algorithms are tasked to 
recognize patterns. Some ingredients are gradient descent, convolution, pooling, weights, receptive field. The wikipedia
explains some categories as building block, these are convolutional layers, pooling layer, ReLu layer, fully connected 
layer and Loss layer. 

**Pooling** is when we partition the set into partitions into sets of non-overlapping rectangles and for each sub-region
outputs the maximum. The idea behind this is that the exact location of a feature is less important than it's rough position.
Take the handwritten number as an example, there will be differences for each 0 or 4 and so on. Max pooling is now the most
common pooling technique and was until recently the most common one until max pooling recently took over.

**Backpropagation** is used to calculate how much each weight contributed to the loss by calculating the gradient of the
 loss with respect to each of them.
 
**Loss layer** specifies how training penalizes the deviation between the predicted output and the true labels. There are
several types of loss functions for different tasks. Softmax loss is used for predicting a single class of K mutex classes.
Sigmoid cross-entropy loss is used for predicting K independent probability values in [0,1]. Euclidean loss is used for 
regressing to real-valued labels [-infinity, infinity]. 

**Fully Connected** Layers are layers where every neuron is connected to every neuron in another layer. 

**ReLu** is the abbreviation on rectified linear unit, which applies the non saturating activation function f(x) = 
max(0,x). It effectively removes negative values from an activation map by setting them to zero. It increases nonlinear
properties of the decision function and of all the overall network without affecting the receptive fields of the
convolution layer

#### Etter svar på melding fra lærer

**Neurons**

**Activation Functions**

**Organization in layers**

**Input, output**
 

## Can you explain a bit about how neural networks are trained
1. Start with a neural network that has trainable parameters(weights)
2. Collect a batch of training data with corresponding labels, X and y
3. Put a batch X into the network and collect the output predictions y_pred at the output layer.
4. Use the loss function to measure the discrepancy between the known labels y and the predictions y_pred for this batch
5. Update all the weights to reduce the discrepancy for this batch:
    1. Calculate each weight's contribution to the loss, in other words, find the gradients of the weights, by using 
    backpropagation.
    2. Move each weight a little bit in the opposite direction of its gradient(gradient descent). weight = weight - learning_rate * Gradient.
6. Do this over and over for several epochs (one epoch is one pass through all the training data) 